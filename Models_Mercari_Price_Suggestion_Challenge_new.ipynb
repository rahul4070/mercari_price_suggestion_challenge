{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models_Mercari_Price_Suggestion_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqppc2r3xyjS"
      },
      "source": [
        "# import modules\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "import math\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import os\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import csv\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZ_7zzMhJtI",
        "outputId": "5c62e674-4b44-431a-f2a0-6d1b62c2930f"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_log_error as rmsle\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from multiprocessing import  Pool\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# importing the .py file which contains the dataCleaning, dataProcessing and converting to vectors\n",
        "import preprocess_data_clean as pde  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/rahul_rbbisht1050/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/rahul_rbbisht1050/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c61FzHdItExP"
      },
      "source": [
        "# !wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kagglesdsdata/competitions/7559/44327/test.tsv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1625901261&Signature=nQ81vqsNNxCnfF2jjUxcwQfN3zPAla934vi2qcjyZtlxWDJRMpuI9Bojlwj1Yc8qwLIXbBfCxLOsPbvjETMO3ed1EMI%2BuueARI%2FcYW5CZ0BaD58GPt1uSkOMNtc2aZXWNIyE%2FzMFuffDE2ZrvBA69zeoZWFO%2BcWZNUsLMjAEl8Iym%2BENUqO8dPx%2BodJNSNmQHXHXetAxmPf%2BII7RxRP7%2BxsL4fDxRFflPeBzbhF%2BayxjtmgznLuh8VTngEewfuWfMs9YgAXE8WgQqC20q4S3%2F2xgHr9NZb4g2CWthC6W4xsOgOIKWviW9g6XutCcvZCQTFcT4ZmIyJEC1H0zgJUTXQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.tsv.7z\" -c -O 'test.tsv.7z'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "U7ZX1xI2tExR",
        "outputId": "70b07fdb-1ee2-4f78-8d55-6a3fcc4ef8f6"
      },
      "source": [
        "# !wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kagglesdsdata/competitions/7559/44327/test_stg2.tsv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1625896403&Signature=qPoxBWeRrpSeWC7nE5E%2BNmpKyfzVNQty3XFEzSqbOUHuXUHXaMFLpoRWAJrqtr3wHujYe9%2B310GfoPy6tbfgUuHIjV6hvUYTG5dI%2BgeTtLmuumCFxKNAhvnd3hqbT7lCggv57iZQbmjS7LesPhLHipVKqSX5%2FTD9ljV4lr5mFxOrPSeqFKtcW%2B8Oq31qdb23USXWJ8CmYPJEgjckGXRqB3X%2Bsunzb571r7%2FL4rM5bYKB7al0u%2FQf9OEm219OKPKesPF9TpafPEKB3pyTdzqx9GFLkl7%2F3Bp1LNIyYXMDX8oGJhWcHgvxYENT5NThoKdC6GNLUA8by2cnbLtLL5mdJw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest_stg2.tsv.zip\" -c -O 'test_stg2.tsv.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-07 07:04:49--  https://storage.googleapis.com/kagglesdsdata/competitions/7559/44327/test_stg2.tsv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1625896403&Signature=qPoxBWeRrpSeWC7nE5E%2BNmpKyfzVNQty3XFEzSqbOUHuXUHXaMFLpoRWAJrqtr3wHujYe9%2B310GfoPy6tbfgUuHIjV6hvUYTG5dI%2BgeTtLmuumCFxKNAhvnd3hqbT7lCggv57iZQbmjS7LesPhLHipVKqSX5%2FTD9ljV4lr5mFxOrPSeqFKtcW%2B8Oq31qdb23USXWJ8CmYPJEgjckGXRqB3X%2Bsunzb571r7%2FL4rM5bYKB7al0u%2FQf9OEm219OKPKesPF9TpafPEKB3pyTdzqx9GFLkl7%2F3Bp1LNIyYXMDX8oGJhWcHgvxYENT5NThoKdC6GNLUA8by2cnbLtLL5mdJw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest_stg2.tsv.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.196.208, 216.58.221.48, 172.217.160.240, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.196.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308669128 (294M) [application/zip]\n",
            "Saving to: ‘test_stg2.tsv.zip’\n",
            "\n",
            "test_stg2.tsv.zip   100%[===================>] 294.37M  35.8MB/s    in 8.2s    \n",
            "\n",
            "2021-07-07 07:04:58 (35.8 MB/s) - ‘test_stg2.tsv.zip’ saved [308669128/308669128]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8gArIVmtExT",
        "outputId": "bef38673-ab78-412f-9ad4-0a25bb5e8b0f"
      },
      "source": [
        "!unzip test_stg2.tsv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test_stg2.tsv.zip\n",
            "  inflating: test_stg2.tsv           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "k4pegsyitExU",
        "outputId": "c77dd36d-725a-4f6b-a84b-bdab2b607d52"
      },
      "source": [
        "# test data for submission\n",
        "\n",
        "test2 = pd.read_csv('test_data/test_stg2.tsv', sep='\\t')\n",
        "print(test2.shape)\n",
        "test2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3460725, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_id</th>\n",
              "      <th>name</th>\n",
              "      <th>item_condition_id</th>\n",
              "      <th>category_name</th>\n",
              "      <th>brand_name</th>\n",
              "      <th>shipping</th>\n",
              "      <th>item_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Breast cancer \"I fight like a girl\" ring</td>\n",
              "      <td>1</td>\n",
              "      <td>Women/Jewelry/Rings</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Size 7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers</td>\n",
              "      <td>1</td>\n",
              "      <td>Other/Office supplies/Shipping Supplies</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Coach bag</td>\n",
              "      <td>1</td>\n",
              "      <td>Vintage &amp; Collectibles/Bags and Purses/Handbag</td>\n",
              "      <td>Coach</td>\n",
              "      <td>1</td>\n",
              "      <td>Brand new coach bag. Bought for [rm] at a Coac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Floral Kimono</td>\n",
              "      <td>2</td>\n",
              "      <td>Women/Sweaters/Cardigan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>-floral kimono -never worn -lightweight and pe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Life after Death</td>\n",
              "      <td>3</td>\n",
              "      <td>Other/Books/Religion &amp; Spirituality</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Rediscovering life after the loss of a loved o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   test_id                                      name  item_condition_id  \\\n",
              "0        0  Breast cancer \"I fight like a girl\" ring                  1   \n",
              "1        1  25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers                  1   \n",
              "2        2                                 Coach bag                  1   \n",
              "3        3                             Floral Kimono                  2   \n",
              "4        4                          Life after Death                  3   \n",
              "\n",
              "                                    category_name brand_name  shipping  \\\n",
              "0                             Women/Jewelry/Rings        NaN         1   \n",
              "1         Other/Office supplies/Shipping Supplies        NaN         1   \n",
              "2  Vintage & Collectibles/Bags and Purses/Handbag      Coach         1   \n",
              "3                         Women/Sweaters/Cardigan        NaN         0   \n",
              "4             Other/Books/Religion & Spirituality        NaN         1   \n",
              "\n",
              "                                    item_description  \n",
              "0                                             Size 7  \n",
              "1  25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined...  \n",
              "2  Brand new coach bag. Bought for [rm] at a Coac...  \n",
              "3  -floral kimono -never worn -lightweight and pe...  \n",
              "4  Rediscovering life after the loss of a loved o...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGxeLz2RtExX",
        "outputId": "1f0601a5-1fb4-4224-ebf6-2f3f5c0b0a42"
      },
      "source": [
        "# getting the dtypes of columns present in feature\n",
        "test2.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3460725 entries, 0 to 3460724\n",
            "Data columns (total 7 columns):\n",
            " #   Column             Dtype \n",
            "---  ------             ----- \n",
            " 0   test_id            int64 \n",
            " 1   name               object\n",
            " 2   item_condition_id  int64 \n",
            " 3   category_name      object\n",
            " 4   brand_name         object\n",
            " 5   shipping           int64 \n",
            " 6   item_description   object\n",
            "dtypes: int64(3), object(4)\n",
            "memory usage: 184.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvknNUYptExZ"
      },
      "source": [
        "# Preprocessing the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibRFl9NmtExc"
      },
      "source": [
        "preprocessDataCleaningFeatureEngineering imported as pde contains a class which has 3 fuctions -<br>\n",
        "preprocess()  - for preprocessing the data <br>\n",
        "calculate_unique_brands() - for finding the unique brands. This is useful for filling the brands value. <br>\n",
        "feature_engineering() - function for feature engineering<br>\n",
        "vectorization() - function to convert features into vectors. <br>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2bW66HetExh"
      },
      "source": [
        "# https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1\n",
        "    \n",
        "def parallelize_dataframe(df, func, n_cores=8):\n",
        "    '''\n",
        "        function to split the dataframe in to chunks and parallelizing those chunks based on the number of cores present.\n",
        "        input: dataframe, function_to_be applied on chunks, n_cores: number of cores\n",
        "        output: combined dataframe from different chunks\n",
        "    '''\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC6ZJIt3tExi"
      },
      "source": [
        "# calling the class present inside the preprocessDataCleaningFeatureEngineering.py file\n",
        "pde_class = pde.DataCleaning_Preprocessing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WBTosJ9ztExl"
      },
      "source": [
        "# calling the preprocess function present inside the class and parallelizing the dataframe\n",
        "test2_preprocess = parallelize_dataframe(test2, pde_class.preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2SsqDivtExn"
      },
      "source": [
        "# function to calculate the unique brands present\n",
        "pde_class.calculate_unique_brands(test2_preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WBP9CgxutExp"
      },
      "source": [
        "# calling the feature_engineering function present inside the class\n",
        "test2_feature_engineering = parallelize_dataframe(test2_preprocess, pde_class.feature_engineering)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeaGDW9dtExq"
      },
      "source": [
        "The shape of test data set is (3460725, 7). It took about 1 hour 17 minutes to run the above cell parallelly on a 8 core cpu in \n",
        "GCP (google cloud platform)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1wSv92utExr"
      },
      "source": [
        "# test2_feature_engineering.to_csv('mercari/test2_feature_engineering.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNe-p_hgtExs"
      },
      "source": [
        "There are some of the datapoints where the text feature are written in japanese language. After preprocessing the text feature\n",
        "it returns the empty string for the respective datapoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y33al_VKtExs"
      },
      "source": [
        "# filling the Nan datapoints which we got as a result of preprocessing\n",
        "test2_feature_engineering.name.fillna('unk_name', inplace=True)\n",
        "test2_feature_engineering.item_description.fillna('unk_desc', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAPgyybmtExt"
      },
      "source": [
        "# reading the test csv file\n",
        "test2_feature_engineering = pd.read_csv('mercari/test2_feature_engineering.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Hjof3utExt"
      },
      "source": [
        "def vectorization(data):\n",
        "\n",
        "    '''\n",
        "        function to convert the data into its respective vector form\n",
        "        input - dataframe\n",
        "        output - csr_matrix with horizontal stacking of features\n",
        "    '''\n",
        "    print('\\n')\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    print('*'*20 + 'Converting the features into vectors...' + '*' * 20)\n",
        "    print()\n",
        "\n",
        "    print(data.columns)\n",
        "\n",
        "    import os.path\n",
        "    if not os.path.isfile('glove.6B.100d.txt'):\n",
        "        print('please download glove vector first!')\n",
        "        return  \n",
        "\n",
        "    print('loading the glove vector...')\n",
        "    words_dict = dict()\n",
        "    f = open('glove.6B.100d.txt')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        words_dict[word] = coefs\n",
        "    f.close()\n",
        "    print('glove vector loaded!')\n",
        "\n",
        "\n",
        "    def sentence_word2vec(sentence):\n",
        "        '''\n",
        "            function to convert text_feature to respective vector form using glove_vector\n",
        "        '''\n",
        "        vector = np.zeros(100)\n",
        "        for word in sentence.split():\n",
        "            if word in words_dict:\n",
        "                vector += words_dict[word]\n",
        "\n",
        "        return vector\n",
        "\n",
        "\n",
        "    print('converting the concatenation of name_description column to its respective vector form...')\n",
        "    name_desc_vector = data.name_desc.astype(str).progress_apply(lambda x: sentence_word2vec(x))\n",
        "    print('\\nconversion of name_desc to vector completed!')\n",
        "    print('-' * 100)\n",
        "    print()\n",
        "\n",
        "    print('converting the concatenation of name, brand, sub_categories column to its respective vector form...')\n",
        "    name_brand_cat_vector = data.name_brand_cat.astype(str).progress_apply(lambda x: sentence_word2vec(x))\n",
        "    print('\\nconversion of name_brand_cat to vector completed!')\n",
        "    print('-' * 100)\n",
        "    print()\n",
        "\n",
        "    data.drop(columns=['name_desc', 'name_brand_cat'], inplace=True)\n",
        "\n",
        "    print('converting text_feature to vector form...')\n",
        "    name_desc_vector = sparse.csc_matrix(name_desc_vector.values.tolist())\n",
        "    name_brand_cat_vector = sparse.csc_matrix(name_brand_cat_vector.values.tolist())\n",
        "    print('converted text features to vector form')\n",
        "    print('-'*100)\n",
        "    print()\n",
        "\n",
        "    cheap_brand = pickle.load(open('cheap_brand_set', \"rb\"))\n",
        "    affordable_brand = pickle.load(open('affordable_brand_set', \"rb\"))\n",
        "    expensive_brand = pickle.load(open('expensive_brand_set', \"rb\"))\n",
        "\n",
        "    def fill_brand_category(brand_name):\n",
        "        try:\n",
        "            if brand_name in cheap_brand:\n",
        "                return 'cheap'\n",
        "            elif brand_name in affordable_brand:\n",
        "                return 'affordable'\n",
        "            elif brand_name in expensive_brand:\n",
        "                return 'expensive'\n",
        "            else:\n",
        "                return 'affordable'\n",
        "        except:\n",
        "            return 'affordable'\n",
        "\n",
        "\n",
        "    data['categorise_brand'] = data['brand_name'].apply(lambda x: fill_brand_category(x))\n",
        "\n",
        "\n",
        "    def ordinal_encoder_load(feature_name, file_name):\n",
        "        encoder = pickle.load(open('encoder/' + file_name + '_ordinal_encoder.pkl', 'rb'))\n",
        "        encoder_values = encoder.transform(data[feature_name].astype(str).values.reshape(-1,1))\n",
        "\n",
        "        imputer = pickle.load(open('imputer/' + file_name + '_imputer.pkl', 'rb'))\n",
        "        imputer_values = imputer.transform(encoder_values)\n",
        "        return imputer_values\n",
        "\n",
        "    print('-'*100)\n",
        "    print('applying label encoder and scaling the features...')\n",
        "\n",
        "\n",
        "    train_brand_name = ordinal_encoder_load('brand_name', 'brand_name')\n",
        "    train_category_brand = ordinal_encoder_load('categorise_brand', 'categorise_brand')\n",
        "    train_category = ordinal_encoder_load('category_name', 'category_name')\n",
        "    train_main_category = ordinal_encoder_load('main_category', 'main_category')\n",
        "    train_sub_category1 = ordinal_encoder_load('brand_name', 'sub_category1')\n",
        "    train_sub_category2 = ordinal_encoder_load('brand_name', 'sub_category2')\n",
        "\n",
        "    # scaling the label encoded features\n",
        "    brand_scaler = pickle.load(open('scaler/brand_scaler.pkl', 'rb'))\n",
        "    train_brand_name =  brand_scaler.transform(train_brand_name)\n",
        "\n",
        "    category_brand_scaler = pickle.load(open('scaler/category_brand_scaler.pkl', \"rb\"))\n",
        "    train_category_brand  = category_brand_scaler.transform(train_category_brand)\n",
        "\n",
        "    category_scaler = pickle.load(open('scaler/category_scaler.pkl', 'rb'))\n",
        "    train_category =  category_scaler.transform(train_category)\n",
        "\n",
        "    main_category_scaler = pickle.load(open('scaler/main_category_scaler.pkl', 'rb'))\n",
        "    train_main_category =  main_category_scaler.transform(train_main_category)\n",
        "\n",
        "    main_sub_category1_scaler = pickle.load(open('scaler/main_sub_category1_scaler.pkl', 'rb'))\n",
        "    train_sub_category1 =  main_sub_category1_scaler.transform(train_sub_category1)\n",
        "\n",
        "    main_sub_category2_scaler = pickle.load(open('scaler/main_sub_category2_scaler.pkl', 'rb'))\n",
        "    train_sub_category2 =  main_sub_category2_scaler.transform(train_sub_category2)\n",
        "\n",
        "    train_brand_name = sparse.csr_matrix(train_brand_name)\n",
        "    train_category_brand = sparse.csr_matrix(train_category_brand)\n",
        "    train_category = sparse.csr_matrix(train_category)\n",
        "    train_main_category = sparse.csr_matrix(train_main_category)\n",
        "    train_sub_category1 = sparse.csr_matrix(train_sub_category1)\n",
        "    train_sub_category2 = sparse.csr_matrix(train_sub_category2)\n",
        "\n",
        "    print('label encoding and scaling of features done!')\n",
        "    print('-'*100)\n",
        "\n",
        "    data.drop(columns=['brand_name','category_name', 'main_category', 'sub_category1', 'sub_category2'], inplace=True)\n",
        "\n",
        "    data['item_condition_id'] = data['item_condition_id'] / 5.0\n",
        "\n",
        "    len_name_scaler = pickle.load(open('scaler/len_name_scaler.pkl', 'rb'))\n",
        "    data['len_name'] =  len_name_scaler.transform(data['len_name'].values.reshape(-1,1))\n",
        "\n",
        "    len_item_description_scaler = pickle.load(open('scaler/len_item_description_scaler.pkl', 'rb'))\n",
        "    data['len_item_description'] =  len_item_description_scaler.transform(data['len_item_description'].values.reshape(-1,1))\n",
        "\n",
        "    print('feature scaling done!')\n",
        "\n",
        "\n",
        "\n",
        "    data_hstack = sparse.hstack((data['item_condition_id'].values.reshape(-1,1),data['shipping'].values.reshape(-1,1),\\\n",
        "                                train_brand_name, train_category_brand, train_category, train_main_category,\\\n",
        "                                train_sub_category1, train_sub_category2, data['len_name'].values.reshape(-1,1),\\\n",
        "                                data['len_item_description'].values.reshape(-1,1), data['brand_value'].values.reshape(-1,1),\\\n",
        "                                data['pos'].values.reshape(-1,1),\\\n",
        "                                data['neg'].values.reshape(-1,1), data['neu'].values.reshape(-1,1), name_desc_vector, \\\n",
        "                                name_brand_cat_vector)).tocsr()\n",
        "\n",
        "    print('time taken to execute the cell : ', datetime.datetime.now()- start_time)\n",
        "    print()\n",
        "\n",
        "\n",
        "    print('*'*30 + 'feature vectorization is done!' + '*' * 30)\n",
        "\n",
        "    return data_hstack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qW7BjOVEtExx",
        "outputId": "c843ce3a-399b-447d-a24c-e6025eb7dcdb"
      },
      "source": [
        "# calling the vectorization function present inside the class\n",
        "text2_vectorization = vectorization(test2_feature_engineering)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********************Converting the features into vectors...********************\n",
            "\n",
            "Index(['Unnamed: 0', 'Unnamed: 0.1', 'test_id', 'name', 'item_condition_id',\n",
            "       'category_name', 'brand_name', 'shipping', 'item_description',\n",
            "       'main_category', 'sub_category1', 'sub_category2', 'len_name',\n",
            "       'len_item_description', 'name_desc', 'name_brand_cat', 'brand_value',\n",
            "       'pos', 'neg', 'neu'],\n",
            "      dtype='object')\n",
            "loading the glove vector...\n",
            "glove vector loaded!\n",
            "converting the concatenation of name_description column to its respective vector form...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████| 3460725/3460725 [01:39<00:00, 34770.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "conversion of name_desc to vector completed!\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "converting the concatenation of name, brand, sub_categories column to its respective vector form...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████| 3460725/3460725 [01:07<00:00, 50950.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "conversion of name_brand_cat to vector completed!\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "converting text_feature to vector form...\n",
            "converted text features to vector form\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "applying label encoder and scaling the features...\n",
            "label encoding and scaling of features done!\n",
            "----------------------------------------------------------------------------------------------------\n",
            "feature scaling done!\n",
            "time taken to execute the cell :  0:04:51.855130\n",
            "\n",
            "******************************feature vectorization is done!******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVFaWnegtEx1"
      },
      "source": [
        "# sparse.save_npz('mercari/X_test2_hstack.npz', text2_vectorization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlDlbzx3tEx1"
      },
      "source": [
        "# loading the vectorizer\n",
        "X_test2 =  sparse.load_npz('mercari/X_test2_hstack.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqiThxYINp9y"
      },
      "source": [
        "# Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojAbwO2JtEx2"
      },
      "source": [
        "data = pd.read_csv('mercari/train1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5QafC7DE9y3"
      },
      "source": [
        "price_by_brand_median = data.groupby('brand_name').median('price').price.to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIUt3Ul2h3Qd"
      },
      "source": [
        "data['brand_price_median'] = data.brand_name.apply(lambda x: price_by_brand_median[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue1rVaq6wAop",
        "outputId": "4b122f1c-cd3f-4aeb-c856-8ca46ff12be2"
      },
      "source": [
        "rmsle(data.price, data['brand_price_median'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43286602410041686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwW0DwzcmBhh"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7BB9wpUtEx5"
      },
      "source": [
        "## Loading the sparse array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OazacJWtEx5"
      },
      "source": [
        "# loading the train sparse array\n",
        "X_train = sparse.load_npz('mercari/X_train_hstack.npz')\n",
        "y_train = sparse.load_npz('mercari/y_train.npz').todense().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_XuzMdAtEx5"
      },
      "source": [
        "# loading the test sparse array\n",
        "X_test =  sparse.load_npz('mercari/X_test_hstack.npz' )\n",
        "y_test = sparse.load_npz('mercari/y_test.npz').todense().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqeHfWpgtEx6"
      },
      "source": [
        "y_train = np.log1p(y_train)\n",
        "y_test  = np.log1p(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWJdlG1FtEx6"
      },
      "source": [
        "# without normalizing the price the models performed very bad. \n",
        "# So after some research found that by converting the price to log prices and by normalizing them the model performed well.\n",
        "\n",
        "# Normalizing the price\n",
        "scaler_price = MinMaxScaler()\n",
        "y_train_scaled = scaler_price.fit_transform(y_train)\n",
        "y_test_sclaed =  scaler_price.transform(y_test)\n",
        "pickle.dump(scaler_price, open('scaler/scaler_price.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVEiM2yFtEx6"
      },
      "source": [
        "scaler_price = pickle.load(open('scaler/scaler_price.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgq2onAaeo3W"
      },
      "source": [
        "def hyperparameter_tuning(x, y, model, params, cv, model_name):\n",
        "    '''\n",
        "        The function is used for finding out the hyperparameters using RandomSearchCV.\n",
        "        input: data_points(x,y) to which the model has to fit, \n",
        "                model: linear_model for which the parameters are to be found,\n",
        "                params: parameters ,\n",
        "                cv: cross validation which is of int type,\n",
        "                model_name: model name of string type \n",
        "\n",
        "        returns: best_hyperparameters\n",
        "    '''\n",
        "    start_time = datetime.datetime.now()\n",
        "    clf = RandomizedSearchCV(estimator = model, param_distributions = params,n_jobs=6, cv=cv, random_state=42, return_train_score=True)\n",
        "    clf.fit(x,y)\n",
        "    print('Calculating the parameters for {} model'.format(model_name))\n",
        "    print()\n",
        "    print('-'*40)\n",
        "    print('best hyperparameters are:-',clf.best_params_)\n",
        "    \n",
        "    print()\n",
        "    print('time taken - ',datetime.datetime.now() - start_time)\n",
        "\n",
        "    return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEvCt3cltEx7"
      },
      "source": [
        "# using the root_mean_squared_error and not the rmsle because the prices are converted to log values\n",
        "def root_mean_squared_error(y_true, y_true_pred):\n",
        "    '''\n",
        "        function to calculate the root mean squared error\n",
        "    '''\n",
        "    return np.sqrt(mean_squared_error(y_true, y_true_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qH3R_eDQBiN"
      },
      "source": [
        "### SGDRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60shHTMC82tv",
        "scrolled": true,
        "outputId": "6f369755-e0ba-44fc-ba16-9638e0de670d"
      },
      "source": [
        "model = SGDRegressor()\n",
        "\n",
        "params = dict()\n",
        "params['alpha'] = loguniform(1e-7, 10)\n",
        "best_params = hyperparameter_tuning(X_train, y_train_scaled, model, params, 3, 'SGDRegressor') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1228: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Calculating the parameters for SGDRegressor model\n",
            "\n",
            "----------------------------------------\n",
            "best hyperparameters are:- {'alpha': 4.033800832600378}\n",
            "\n",
            "time taken -  2:08:53.203130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogdQuISGMjUJ"
      },
      "source": [
        "sgd_regressor_model = SGDRegressor(alpha=4.033800832600378)\n",
        "sgd_regressor_model.fit(X_train, y_train_scaled)\n",
        "y_train_pred = sgd_regressor_model.predict(X_train)\n",
        "y_test_pred  = sgd_regressor_model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM_65qo3tEx8"
      },
      "source": [
        "y_train_pred = scaler_price.inverse_transform(y_train_pred.reshape(-1,1))\n",
        "y_test_pred =  scaler_price.inverse_transform(y_test_pred.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "TAEQbNFEtEx8",
        "outputId": "55c135cf-2a39-434b-b2c5-0df03e47a796"
      },
      "source": [
        "print('Train error: ',root_mean_squared_error(y_train, y_train_pred))\n",
        "print('Test  error: ',root_mean_squared_error(y_test,  y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train error:  1.2693467728931624\n",
            "Test  error:  1.2723025954460385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jLcu7QHtEx9"
      },
      "source": [
        "# saving the model\n",
        "pickle.dump(sgd_regressor_model, open('models/sgd_regressor_model.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Futm2ZHutEx9"
      },
      "source": [
        "# loaidng the model\n",
        "sgd_regressor = pickle.load(open('models/sgd_regressor_model.pkl', 'rb'))\n",
        "\n",
        "pred1_test = sgd_regressor.predict(X_test2)\n",
        "\n",
        "submission_sgd = pd.DataFrame(test2.test_id, columns=['test_id'])\n",
        "\n",
        "submission_sgd['price'] = np.expm1(scaler_price.inverse_transform(pred1_test.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbqdD25ctEx9"
      },
      "source": [
        "submission_sgd.to_csv('submission/submission_sgd.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbGbyZr6QF9z"
      },
      "source": [
        "### Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAbdFtzaQPGY",
        "scrolled": true,
        "outputId": "10cb5b3d-1681-46b5-da6d-0f373a26840c"
      },
      "source": [
        "model = Lasso()\n",
        "\n",
        "params = dict()\n",
        "params['alpha'] = [0.0000001, 0.00001, 0.001, 0.1, 1, 3, 5, 7, 9]\n",
        "hyperparameters_ = hyperparameter_tuning(X_train, y_train_scaled, model, params, 3, 'Lasso Regressor')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:516: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 633.2432479492218, tolerance: 1.2752720590094584\n",
            "  max_iter, tol, rng, random, positive)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:516: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321.31156095767165, tolerance: 1.280136329427535\n",
            "  max_iter, tol, rng, random, positive)\n",
            "/home/rahul_rbbisht1050/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:516: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 613.4679720904151, tolerance: 1.2790246086164758\n",
            "  max_iter, tol, rng, random, positive)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Calculating the parameters for Lasso Regressor model\n",
            "\n",
            "----------------------------------------\n",
            "best hyperparameters are:- {'alpha': 1e-07}\n",
            "\n",
            "time taken -  1:14:55.094241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCawNiH3QPCe"
      },
      "source": [
        "lasso_regressor_model = Lasso(alpha=1e-07)\n",
        "lasso_regressor_model.fit(X_train,y_train_scaled)\n",
        "y_train_pred = lasso_regressor_model.predict(X_train)\n",
        "y_test_pred  = lasso_regressor_model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwpkJp_etEx_"
      },
      "source": [
        "y_train_pred = scaler_price.inverse_transform(y_train_pred.reshape(-1,1))\n",
        "y_test_pred =  scaler_price.inverse_transform(y_test_pred.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1E01DxVatEx_",
        "outputId": "cff0e65c-bb3b-4c94-8dd9-8d1cb07f9d0f"
      },
      "source": [
        "print('Train error: ',root_mean_squared_error(y_train, y_train_pred))\n",
        "print('Test  error: ',root_mean_squared_error(y_test,  y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train error:  0.6494764360543506\n",
            "Test  error:  0.6487308139438533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj3X7biNtEyA"
      },
      "source": [
        "pickle.dump(lasso_regressor_model, open('models/lasso_regressor_model.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0g5_y2CtEyA"
      },
      "source": [
        "lasso_regressor = pickle.load(open('models/lasso_regressor_model.pkl', 'rb'))\n",
        "\n",
        "pred2_test = lasso_regressor.predict(X_test2)\n",
        "# pred2_test =  lasso_regressor.predict(X_test)\n",
        "\n",
        "submission_lasso = pd.DataFrame(test2.test_id, columns=['test_id'])\n",
        "\n",
        "submission_lasso['price'] = np.expm1(scaler_price.inverse_transform(pred2_test.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8772kzOtEyB"
      },
      "source": [
        "submission_lasso.to_csv('submission/submission_lasso.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCz9wc46SeVs"
      },
      "source": [
        "### Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEk0PPRfSeVs",
        "outputId": "86894e32-ee29-421b-e56c-b5a95d0db765"
      },
      "source": [
        "model = Ridge(fit_intercept=False)\n",
        "\n",
        "params = dict()\n",
        "params['solver'] = ['cholesky','lsqr']              \n",
        "params['alpha'] = loguniform(1e-7, 12)\n",
        "best_params = hyperparameter_tuning(X_train, y_train_scaled, model, params, 3, 'Ridge_Regressor')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating the parameters for Ridge_Regressor model\n",
            "\n",
            "----------------------------------------\n",
            "best hyperparameters are:- {'alpha': 0.00686547196906466, 'solver': 'cholesky'}\n",
            "\n",
            "time taken -  0:08:07.860483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWcXzB3GSeVt"
      },
      "source": [
        "ridge_regressor_model = Ridge(alpha=0.00686547196906466,solver='lsqr', fit_intercept=False)\n",
        "ridge_regressor_model.fit(X_train,y_train_scaled)\n",
        "y_train_pred = ridge_regressor_model.predict(X_train)\n",
        "y_test_pred  = ridge_regressor_model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_tLVCgQtEyC"
      },
      "source": [
        "y_train_pred = scaler_price.inverse_transform(y_train_pred.reshape(-1,1))\n",
        "y_test_pred =  scaler_price.inverse_transform(y_test_pred.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "G_jWy2qutEyD",
        "outputId": "233c4576-219b-4d46-e68f-dc9c9dcbe9e7"
      },
      "source": [
        "print('Train error: ',root_mean_squared_error(y_train, y_train_pred))\n",
        "print('Test  error: ',root_mean_squared_error(y_test,  y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train error:  0.6622761382924385\n",
            "Test  error:  0.6618423662958506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNV6KA3mtEyD"
      },
      "source": [
        "pickle.dump(ridge_regressor_model, open('models/ridge_regressor_model.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMHqUJb3tEyD"
      },
      "source": [
        "ridge_regressor = pickle.load(open('models/ridge_regressor_model.pkl', 'rb'))\n",
        "\n",
        "pred3_test = ridge_regressor.predict(X_test2)\n",
        "# pred3_test =  ridge_regressor.predict(X_test)\n",
        "\n",
        "submission_ridge = pd.DataFrame(test2.test_id, columns=['test_id'])\n",
        "\n",
        "submission_ridge['price'] = np.expm1(scaler_price.inverse_transform(pred3_test.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wh_-dpytEyE"
      },
      "source": [
        "submission_ridge.to_csv('submission/submission_ridge.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F525Pd5ZUHGU"
      },
      "source": [
        "### LGBM Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP69tcjDtEyE"
      },
      "source": [
        "from lightgbm import LGBMRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4pDOqC6UHGm",
        "outputId": "c365cf5a-b6db-4bbd-e086-b9f834183920"
      },
      "source": [
        "params = dict()\n",
        "params={'learning_rate':[0.1,0.3,0.5,0.6],'max_depth':[5,8,12,15],'n_estimators':[50,100,150,200],'num_leaves':[15,25,50,75],'boosting_type':['gbdt']}\n",
        "lgbm_params={'sub_sample':0.9,'colsample_bytree':0.8,'min_child_samples':50,'objective':'regression'}\n",
        "model = LGBMRegressor(**lgbm_params)\n",
        "\n",
        "\n",
        "best_params = hyperparameter_tuning(X_train, y_train_scaled, model, params, 3, 'LGBM Regressor')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: sub_sample\n",
            "Calculating the parameters for LGBM Regressor model\n",
            "\n",
            "----------------------------------------\n",
            "best hyperparameters are:- {'num_leaves': 75, 'n_estimators': 100, 'max_depth': 12, 'learning_rate': 0.5, 'boosting_type': 'gbdt'}\n",
            "\n",
            "time taken -  0:17:21.283378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5yE4nkhUHGn",
        "outputId": "aceccce0-5517-4b18-85ee-2287baf7add9"
      },
      "source": [
        "params = {'num_leaves': 75, 'n_estimators': 100, 'max_depth': 12, 'learning_rate': 0.5, 'boosting_type': 'gbdt'}\n",
        "lgbm_params={'sub_sample':0.9,'colsample_bytree':0.8,'min_child_samples':50,'objective':'regression'}\n",
        "lgbm_regressor_model = LGBMRegressor(**params,**lgbm_params)\n",
        "lgbm_regressor_model.fit(X_train,y_train_scaled)\n",
        "y_train_pred = lgbm_regressor_model.predict(X_train)\n",
        "y_test_pred  = lgbm_regressor_model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: sub_sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stSEWvnItEyF"
      },
      "source": [
        "y_train_pred = scaler_price.inverse_transform(y_train_pred.reshape(-1,1))\n",
        "y_test_pred =  scaler_price.inverse_transform(y_test_pred.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jfsvH5bStEyF",
        "outputId": "851878ba-d0f1-4993-a59f-57078b50b3af"
      },
      "source": [
        "print('Train error: ',root_mean_squared_error(y_train, y_train_pred))\n",
        "print('Test  error: ',root_mean_squared_error(y_test,  y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train error:  0.5081683785557494\n",
            "Test  error:  0.5264128509090326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9byj2OUtEyF"
      },
      "source": [
        "pickle.dump(lgbm_regressor_model, open('models/lgbm_regressor_model.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijS3VWW4tEyG"
      },
      "source": [
        "lgbm_regressor = pickle.load(open('models/lgbm_regressor_model.pkl', 'rb'))\n",
        "\n",
        "pred4_test = lgbm_regressor.predict(X_test2)\n",
        "# pred4_test =  lgbm_regressor.predict(X_test)\n",
        "\n",
        "submission_lgbm = pd.DataFrame(test2.test_id, columns=['test_id'])\n",
        "\n",
        "submission_lgbm['price'] = np.expm1(scaler_price.inverse_transform(pred4_test.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWtG5kOptEyG"
      },
      "source": [
        "submission_lgbm.to_csv('submission/submission_lgbm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdnPToEztEyG",
        "outputId": "44510488-bb5e-403e-e11d-addf030bb458"
      },
      "source": [
        "\n",
        "from prettytable import PrettyTable\n",
        "  \n",
        "# Specify the Column Names while initializing the Table\n",
        "x = PrettyTable([\"model\", \"train score\", \"test score\", \"submission score\"])\n",
        "  \n",
        "# Add rows\n",
        "x.add_row([\"SGD Regressor\", \"1.26934\", \"1.27230\", \"1.21826\"])\n",
        "x.add_row([\"Lasso Regressor\", \"0.64947\", \"0.64873\", \"2.57205\", ])\n",
        "x.add_row([\"Ridge Regressor\", \"0.50816\", \"0.52641\", \"0.71395\"])\n",
        "x.add_row([\"LGBM Regressor\", \"0.50816\", \"0.52641\", \"0.70646\"])\n",
        "print()\n",
        "print()\n",
        "print(x)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "+-----------------+-------------+------------+------------------+\n",
            "|      model      | train score | test score | submission score |\n",
            "+-----------------+-------------+------------+------------------+\n",
            "|  SGD Regressor  |   1.26934   |  1.27230   |     1.21826      |\n",
            "| Lasso Regressor |   0.64947   |  0.64873   |     2.57205      |\n",
            "| Ridge Regressor |   0.50816   |  0.52641   |     0.71395      |\n",
            "|  LGBM Regressor |   0.50816   |  0.52641   |     0.70646      |\n",
            "+-----------------+-------------+------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbEkaYcQtEyH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
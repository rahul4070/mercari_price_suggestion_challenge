{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_preprocessing_Mercari_Price_Suggestion_Challenge_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxNSQ7EKoKNK"
      },
      "source": [
        "# 1.Buisness Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_-Wi5rz5MwK"
      },
      "source": [
        "#### **What is Mercari ?**\n",
        "\n",
        "> Mercari is a marketplace platform where you can buy and sell almost anything provided the item can be shipped. It is currently operating in Japan and the United States. It is similar to the Quickr or OLX in India.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2oucAE26sCb"
      },
      "source": [
        "#### **Problem Statement**\n",
        "\n",
        "> Predicting the price of the product can be a tough challenge especially when the product is seasonal. The same product with different brands can range in the different price range making it difficult for the sellers to set a fair price to make profit and selling the product below the market price would be a loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU1WmL5p-uoV"
      },
      "source": [
        "#### **Business Objective**\n",
        "\n",
        "\n",
        "\n",
        ">* Mercari wants us to come up with a model that can suggest the best market price of that product online such that there will be a fair price predicted for the product leading to better experience for the customer and seller<br>\n",
        ">* Model should not take too long to predict the price.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2cwGq-FB8n4"
      },
      "source": [
        "#### **Data Overview**\n",
        "\n",
        "> The files consist of a list of product which are seperated by tab.\n",
        "\n",
        "    * train_id or test_id - the id of the listing\n",
        "\n",
        "    * name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. \\$20) to avoid leakage. \n",
        "      These removed prices are represented as [rm] \n",
        "\n",
        "    * item_condition_id - the condition of the items provided by the seller, range from 1-5. 1 being 'New' and 5 being 'Poor'.\n",
        "\n",
        "    * category_name - category of the listing\n",
        "\n",
        "    * brand_name - brand of the product. \n",
        "\n",
        "    * price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. \n",
        "\n",
        "    * shipping - 1 if shipping fee is paid by seller and 0 by buyer\n",
        "\n",
        "    * item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices\n",
        "     (e.g. \\$20) to avoid leakage. These removed prices are represented as [rm]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7_zBKTiAsjS"
      },
      "source": [
        "#### **Type of Machine Learning problem**\n",
        "\n",
        "> The price is a target variable which is a continuous variable, therefore it is a Regression problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgxzmdC7EtKp"
      },
      "source": [
        "#### **Performance Metric -**\n",
        "\n",
        "> The error metric that we use for the regression problem is the RMSE (root mean square error), MSE(mean abosolute error) and MSE(mean square error). In this case study we are going to use the RMSLE which is Root Mean Squared Logarithmic Error because of the following reasons -<br>\n",
        ">    * Robust to outliers.\n",
        ">    * It scale invariant which means changing the scale by a certain amount does not change the final result.\n",
        ">    * Biased Penalty - penalises those error which are underestimated\n",
        "rather than overestimated which may increase the profit for company and seller as well.\n",
        "\n",
        "**NOTE :** **The above comparisions are with respect to the RMSE only.**\n",
        "<br><br>\n",
        "The RMSLE is calculated as -\n",
        "<img src=\"https://miro.medium.com/max/875/0*AUzyQ1rc6mpQVYfn\">\n",
        "\n",
        "reference - https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOpuRBvnNthI"
      },
      "source": [
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnj0eg8nLV3b"
      },
      "source": [
        "## Importing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqppc2r3xyjS"
      },
      "source": [
        "# import modules\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pickle\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "import math\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import os\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1RVDUuykaa_",
        "scrolled": false,
        "outputId": "8d3a9021-5890-4ecc-b82b-ffc732d69aee"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip - ‘glove.6B.zip’ saved [862182613/862182613]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-21 15:12:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-07-21 15:12:39--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-07-21 15:12:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        3%[                    ]  28.77M  6.24MB/s    eta 2m 7s  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUzNyVrAlVkv",
        "scrolled": true
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNTLlU63wVH8",
        "outputId": "2c33ae3a-8878-4b8e-fb4a-98be5a1ce572"
      },
      "source": [
        "# importing the data in colab using CurlWget\n",
        "\n",
        "! wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kagglesdsdata/competitions/7559/44327/train.tsv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1626025725&Signature=WJlWIDJL0ILb0i0AmBY%2FBkDpfsXWgPLoODrqS%2B%2F%2FGnlmR0B%2F8d1vuuxjjfExL2utyqmH3h96Icw4iG5Xm5vXRGYSHz%2B2PEh0ZWZOkEhracCRzpa5Ymv4RKxJ3beC5rrnCU4Ug24%2FHKjR75H2tFRohRo1fDzGs1MR2VFUS%2BlFnc2tH6TlZVgGtExd0ft%2BhQbc2ywXFlpUsRF7NmPOYzsraoYK8zPTvwQG5w7n7TC5bwEL8ZhTgV9K321%2FND1TRcZdgdtjCA8wTo2qgDvBcoz28i800dO9lskNkTeCfxEghVo1wy6r0z5tn3kxFH8jyW57QbJlBsrJkeoHDjs7aFwHPg%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.tsv.7z\" -c -O 'train.tsv.7z'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-08 17:50:26--  https://storage.googleapis.com/kagglesdsdata/competitions/7559/44327/train.tsv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1626025725&Signature=WJlWIDJL0ILb0i0AmBY%2FBkDpfsXWgPLoODrqS%2B%2F%2FGnlmR0B%2F8d1vuuxjjfExL2utyqmH3h96Icw4iG5Xm5vXRGYSHz%2B2PEh0ZWZOkEhracCRzpa5Ymv4RKxJ3beC5rrnCU4Ug24%2FHKjR75H2tFRohRo1fDzGs1MR2VFUS%2BlFnc2tH6TlZVgGtExd0ft%2BhQbc2ywXFlpUsRF7NmPOYzsraoYK8zPTvwQG5w7n7TC5bwEL8ZhTgV9K321%2FND1TRcZdgdtjCA8wTo2qgDvBcoz28i800dO9lskNkTeCfxEghVo1wy6r0z5tn3kxFH8jyW57QbJlBsrJkeoHDjs7aFwHPg%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.tsv.7z\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.196.112, 216.58.200.176, 216.58.200.208, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.196.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77912192 (74M) [application/x-7z-compressed]\n",
            "Saving to: ‘train.tsv.7z’\n",
            "\n",
            "train.tsv.7z        100%[===================>]  74.30M   302MB/s    in 0.2s    \n",
            "\n",
            "2021-07-08 17:50:27 (302 MB/s) - ‘train.tsv.7z’ saved [77912192/77912192]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gycwn9Q-x9Dl",
        "outputId": "96b29f0d-4981-45ce-ce73-28b90e2a79dc"
      },
      "source": [
        "# reference - https://stackoverflow.com/questions/49955814/unzip-a-7z-file-in-google-collab\n",
        "\n",
        "!7z e train.tsv.7z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip [64] 9.20  Copyright (c) 1999-2010 Igor Pavlov  2010-11-18\n",
            "p7zip Version 9.20 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,8 CPUs)\n",
            "\n",
            "Processing archive: train.tsv.7z\n",
            "\n",
            "Extracting  train.tsv\n",
            "\n",
            "Everything is Ok\n",
            "\n",
            "Size:       337809843\n",
            "Compressed: 77912192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzHabDR6ukqM"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0xruYNHNfUY"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDLSeyHyM9T",
        "outputId": "7932363f-7c62-4ca9-bd27-37770f97d441"
      },
      "source": [
        "# importing the data\n",
        "# here the file is tab-seperated, therefore using '\\t'\n",
        "data = pd.read_csv('train.tsv', sep='\\t')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_id</th>\n",
              "      <th>name</th>\n",
              "      <th>item_condition_id</th>\n",
              "      <th>category_name</th>\n",
              "      <th>brand_name</th>\n",
              "      <th>price</th>\n",
              "      <th>shipping</th>\n",
              "      <th>item_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
              "      <td>3</td>\n",
              "      <td>Men/Tops/T-shirts</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>No description yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
              "      <td>3</td>\n",
              "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
              "      <td>Razer</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0</td>\n",
              "      <td>This keyboard is in great condition and works ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>AVA-VIV Blouse</td>\n",
              "      <td>1</td>\n",
              "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
              "      <td>Target</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Leather Horse Statues</td>\n",
              "      <td>1</td>\n",
              "      <td>Home/Home Décor/Home Décor Accents</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>24K GOLD plated rose</td>\n",
              "      <td>1</td>\n",
              "      <td>Women/Jewelry/Necklaces</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Complete with certificate of authenticity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_id                                 name  item_condition_id  \\\n",
              "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
              "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
              "2         2                       AVA-VIV Blouse                  1   \n",
              "3         3                Leather Horse Statues                  1   \n",
              "4         4                 24K GOLD plated rose                  1   \n",
              "\n",
              "                                       category_name brand_name  price  \\\n",
              "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
              "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
              "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
              "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
              "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
              "\n",
              "   shipping                                   item_description  \n",
              "0         1                                 No description yet  \n",
              "1         0  This keyboard is in great condition and works ...  \n",
              "2         1  Adorable top with a hint of lace and a key hol...  \n",
              "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
              "4         0          Complete with certificate of authenticity  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZimC340Cz7os",
        "scrolled": true,
        "outputId": "685d7a51-6c06-487d-dd8d-181faeb18bfe"
      },
      "source": [
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1482535, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ4DyQ1pSd_g"
      },
      "source": [
        "###### Observation - \n",
        "The data contains near about 1.5 million rows and 8 columns\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrOM0wlzOAwW"
      },
      "source": [
        "# reference - https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "\n",
        "def decontracted(phrase):\n",
        "\n",
        "    '''  \n",
        "        this function helps in expanding the given phrases.\n",
        "\n",
        "        input: phrase/ word\n",
        "        returns: expanded string \n",
        "    '''\n",
        "\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_G5Je7qQO2d"
      },
      "source": [
        "def category_split(string):\n",
        "    '''\n",
        "        function to split the category column into three subcategories\n",
        "\n",
        "        input: category string\n",
        "        returns: three subcategory strings \n",
        "    '''\n",
        "    try:\n",
        "        # split the string with '/'\n",
        "        t = string.split('/')\n",
        "        return t[0], t[1], t[2]\n",
        "    except:\n",
        "        return 'unk_cat', 'unk_cat', 'unk_cat'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fityIO_R0thT",
        "outputId": "8336a3e2-702c-440f-f8ed-119a8e79f500"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/rahul_rbbisht1050/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R6x_K5Flxdv"
      },
      "source": [
        "stopwords_ = stopwords.words('english')\n",
        "\n",
        "def preprocess(text,col):\n",
        "    \"\"\"\n",
        "        Function to clean the strings containing special characters and converts them to lowercase characters.\n",
        "\n",
        "        input: string\n",
        "        output: string which contains number and lower character.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # convert the string to lowercase\n",
        "        text = text.lower()\n",
        "        # decontraction - expanding the words like : i'll -> i will, he'd -> he would\n",
        "        text = decontracted(text)\n",
        "        # replace & and - character with _ . \n",
        "        text = re.sub('[&-]', '_', text)    #  Example : t-shirt -> t_shirt, horse&sweater -> horse_sweater\n",
        "        # replace special characters except _\n",
        "        text = re.sub('[^0-9a-z_]',' ',text)\n",
        "        text = re.sub('\\s_\\s', ' ', text)   #  replace strings like  ' _ ' with ' ' (string with a space)\n",
        "        text = re.sub('\\s+', ' ', text).strip()  # replace more than one_space_character to single_space_character\n",
        "        if col != 'name':\n",
        "            # removing the stopwords\n",
        "            text = ' '.join(i for i in text.split(' ') if not i in stopwords_)\n",
        "        else:\n",
        "            text = ' '.join(i for i in text.split(' '))\n",
        "    except:\n",
        "        text = np.nan\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqLtsLkD1LQY"
      },
      "source": [
        "# reference - https://albertauyeung.github.io/2018/06/03/generating-ngrams.html\n",
        "\n",
        "def generate_ngrams(s, n):\n",
        "    \n",
        "    '''\n",
        "        function to return the ngrams\n",
        "        input: s: sentence, n- ngrams\n",
        "        output: ngrams \n",
        "    '''\n",
        "    \n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "    \n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    # print(list(ngrams))\n",
        "    return [\" \".join(ngram) for ngram in ngrams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmxQQ-3XdKka"
      },
      "source": [
        "def fill_missing_brands(df):\n",
        "    '''\n",
        "        function to fill the missing brands\n",
        "        input: dataframe row\n",
        "        output: if present: brand_name otherwise 'unk_brand'\n",
        "    '''\n",
        "    name, brand_name, item_description = df[0], df[1], df[2]\n",
        "    name = str(name) + ' ' + str(item_description)\n",
        "    ngram_ = [4,3,2,1]\n",
        "    if brand_name != 'unk_brand':\n",
        "        return brand_name\n",
        "    else:\n",
        "        try:\n",
        "            brand_names = []\n",
        "            for i in ngram_:\n",
        "                for grams in generate_ngrams(name, i):\n",
        "                    brand = ' '.join(grams)\n",
        "                    if brand in all_unique_brands:\n",
        "                        brand_names.append(brand)\n",
        "            if len(brand_names) > 0:\n",
        "                return brand_names[0]\n",
        "            else:\n",
        "                return 'unk_brand'\n",
        "        except :\n",
        "            return 'unk_brand'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqbDn7YquXmM",
        "scrolled": false,
        "outputId": "0fbff5c7-10e1-4005-f558-9ba15e37527f"
      },
      "source": [
        "start_time = datetime.datetime.now()\n",
        "\n",
        "print('products whoose price are greater than 0')\n",
        "data = data[data['price'] > 0].reset_index(drop=True)\n",
        "print('products filtering done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "\n",
        "print('filling nan category_name values with \"unk_cat/unk_subcat1/unk_subcat2\"...')\n",
        "data.category_name.fillna('unk_cat/unk_subcat1/unk_subcat2',inplace=True)\n",
        "print('filling nan category_name complete!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "\n",
        "print('converting brand_name to lower case...')\n",
        "data['brand_name'] = data['brand_name'].str.lower()\n",
        "print('brand_name converted to lower case!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('creating subcategory columns...')\n",
        "data['main_category'], data['sub_category1'], data['sub_category2'] = zip(*data.category_name.apply(lambda x: category_split(x)))\n",
        "print('subcategory columns creation completed!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('filling nan of item_description...')\n",
        "data['item_description'].fillna('unk_desc',inplace=True)\n",
        "print('item_description fillna complete !')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('replacing \"No descripiton yet\" with \"unk_desc\"...')\n",
        "data['item_description'] = data['item_description'].str.replace('No description yet', 'unk_desc')\n",
        "print('item_description replacement of \"No description yet\" with \"unk_desc\" complete!')\n",
        "\n",
        "print()\n",
        "print('time taken to execute the cell : ', datetime.datetime.now()- start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "products whoose price are greater than 0\n",
            "products filtering done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "filling nan category_name values with \"unk_cat/unk_subcat1/unk_subcat2\"...\n",
            "filling nan category_name complete!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "converting brand_name to lower case...\n",
            "brand_name converted to lower case!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "creating subcategory columns...\n",
            "subcategory columns creation completed!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "filling nan of item_description...\n",
            "item_description fillna complete !\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "replacing \"No descripiton yet\" with \"unk_desc\"...\n",
            "item_description replacement of \"No description yet\" with \"unk_desc\" complete!\n",
            "\n",
            "time taken to execute the cell :  0:00:06.766591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39yw1rZvzOV6",
        "scrolled": true,
        "outputId": "9c8d667f-1c04-455c-a292-299244291e97"
      },
      "source": [
        "print('preprocessing name...')\n",
        "data['name'] = data['name'].progress_apply(lambda x: preprocess(x, 'name'))\n",
        "print('preprocessing of name complete!')\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|                                                                    | 2668/1482535 [00:00<00:55, 26677.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing name...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████| 1482535/1482535 [00:37<00:00, 39576.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing of name complete!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "p2StiAcvxQh0",
        "outputId": "80452c5b-e7de-44a1-eadf-f9c8431890ea"
      },
      "source": [
        "print('preprocessing item_description...')\n",
        "data['item_description'] = data['item_description'].progress_apply(lambda x: preprocess(x, 'item_description'))\n",
        "print('preprocessing of item_description complete!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing item_description...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████| 1482535/1482535 [02:50<00:00, 8681.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing of item_description complete!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTDBCC3KxQh3",
        "outputId": "3fdffff3-9511-4728-a2a4-280ba99757fd"
      },
      "source": [
        "print('filling nan brand values with \"unk_brand\"...')\n",
        "data.brand_name.fillna('unk_brand',inplace=True)\n",
        "print('\\nfilled nan brand_name!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "no_brand_name_before = data[data.brand_name == 'unk_brand'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filling nan brand values with \"unk_brand\"...\n",
            "\n",
            "filled nan brand_name!\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QdN9BwnkxQh5",
        "outputId": "8a4f465f-ade6-4408-d3f0-a89bcb7385f2"
      },
      "source": [
        "start_time = datetime.datetime.now()\n",
        "\n",
        "print('filling missing brand_name with help of \"name\" feature...')\n",
        "all_unique_brands = data.brand_name.unique()\n",
        "pickle.dump(all_unique_brands, open('all_unique_brands', \"wb\"))\n",
        "all_unique_brands\n",
        "data['brand_name'] = data[['name','brand_name','item_description']].progress_apply(fill_missing_brands, axis=1)\n",
        "print('\\nfill missing brand name complete!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('time taken to execute the cell : ', datetime.datetime.now()- start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                                                                  | 0/1481661 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "filling missing brand_name with help of \"name\" feature...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 52%|██████████████████████████████████▌                               | 775019/1481661 [1:19:09<53:00, 222.21it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "CosPmRpLxQh7"
      },
      "source": [
        "print('number of unk_brand filled- {}'.format(no_brand_name_before - data[data.brand_name == 'unk_brand'].shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ET6CL3xQh7"
      },
      "source": [
        "data_copy = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQb1qWPQxQh8"
      },
      "source": [
        "# after preprocessing of name some of the values are processed and after removing stopwords and special character(can also be \n",
        "# in japnese language)\n",
        "# are made null\n",
        "print('replacing the Nan values with \"unk_name\"')\n",
        "data_copy['name'] = data_copy.name.str.replace('','unk_name')\n",
        "print('filling of nan values complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg-mdu0xxQh8"
      },
      "source": [
        "# after preprocessing of item description some of the values are processed and after removing stopwords and special character\n",
        "# they are null values\n",
        "print('replacing the Nan values with \"unk_desc\"')\n",
        "data_copy['item_description'] = data_copy['item_description'].str.replace('', 'unk_desc')\n",
        "print('filling of nan values complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IaGhlyvuesR"
      },
      "source": [
        "# dataframe to save into disk\n",
        "\n",
        "data_copy.to_csv('mercari/mercari_dataframe_feature_engineering.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDJh705JdlIN"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/mercari/mercari_dataframe.csv',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bnI-SDG3VSR"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NnT_NtlxQh_"
      },
      "source": [
        "data_ = pd.read_csv('mercari/mercari_dataframe_feature_engineering.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjec6rATxQh_"
      },
      "source": [
        "data = data_.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY8qTgoR238b",
        "outputId": "b3bf5636-c26a-47f4-ea21-4c77ede7c0b1"
      },
      "source": [
        "print('\\ncomputing word count of name feature...')\n",
        "data['len_name'] = data['name'].apply(lambda x: len(str(x).split(' ')))\n",
        "print('\\nname_feature word count computation done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('\\ncomputing word count of item description...')\n",
        "data['len_item_description'] = data['item_description'].apply(lambda x: len(str(x).split(' ')))\n",
        "print('\\nitem description word count computation done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('\\ncombining name with item descripiton with word count 10 ...')\n",
        "data['name_desc'] = data['name'] + ' ' + data['item_description'].apply(lambda x: ' '.join(str(x).split(' ')[:10]))\n",
        "print('\\ncombining feature name and item_description done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('\\ncombining name, brand_name, subcategories together...')\n",
        "data['name_brand_cat'] = 'name ' + data['name'] + ' ' + 'brand ' + data['brand_name'] + ' ' + 'main category ' + data['main_category'] + \\\n",
        "                        ' ' + 'sub category ' + data['sub_category1'] + ' ' + 'sub category ' + data['sub_category2']\n",
        "print('\\ncombining feature name, brand_name, subcategories together done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('\\nassigning the branded_products with value 1 and unknown_branded products with 0...')\n",
        "data['brand_value'] = data['brand_name'].apply(lambda x: 1 if x != 'unk_brand' else 0)\n",
        "print('\\nassigning the branded products with 1 and non branded products with 0 done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "computing word count of name feature...\n",
            "\n",
            "name_feature word count computation done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "computing word count of item description...\n",
            "\n",
            "item description word count computation done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "combining name with item descripiton with word count 10 ...\n",
            "\n",
            "combining feature name and item_description done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "combining name, brand_name, subcategories together...\n",
            "\n",
            "combining feature name, brand_name, subcategories together done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "assigning the branded_products with value 1 and unknown_branded products with 0...\n",
            "\n",
            "assigning the branded products with 1 and non branded products with 0 done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-JxxAJlKQPC",
        "outputId": "4a8bd5c4-a792-4bf1-c19b-8e389201a0c4"
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/rahul_rbbisht1050/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faJ-hzOKQnG"
      },
      "source": [
        "def sentiment_analysis(sentence, sentiment):\n",
        "    ss = sid.polarity_scores(sentence)\n",
        "    senti_ = ss[sentiment]\n",
        "\n",
        "    return senti_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cobvJrybKQU0",
        "scrolled": false,
        "outputId": "fc628854-1f83-4f6c-b406-2db425eadf56"
      },
      "source": [
        "print('\\ndoing sentiment_analysis for positive sentences...')\n",
        "data['pos'] = data.item_description.astype(str).progress_apply(lambda x: sentiment_analysis(x, 'pos'))\n",
        "print('\\nsentiment analysis of positive sentences done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('\\ndoing sentiment_analysis for negative sentences...')\n",
        "data['neg'] = data.item_description.astype(str).progress_apply(lambda x: sentiment_analysis(x, 'neg'))\n",
        "print('\\nsentiment analysis of negative sentences done!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('\\ndoing sentiment_analysis for neutral sentences...')\n",
        "data['neu'] = data.item_description.astype(str).progress_apply(lambda x: sentiment_analysis(x, 'neu'))\n",
        "print('\\nsentiment analysis of neutral sentences done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "doing sentiment_analysis for positive sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████| 1481661/1481661 [06:32<00:00, 3771.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "sentiment analysis of positive sentences done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "doing sentiment_analysis for negative sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████| 1481661/1481661 [06:33<00:00, 3767.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "sentiment analysis of negative sentences done!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "doing sentiment_analysis for neutral sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████| 1481661/1481661 [06:31<00:00, 3785.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "sentiment analysis of neutral sentences done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7IhuFsVxQiF",
        "outputId": "cbdb6cad-66ba-4c13-c4d1-2e5fb3c3311a"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1481661, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXSeNrNo3uRR"
      },
      "source": [
        "# dataframe to save into disk\n",
        "\n",
        "data.to_csv('mercari/train1.csv',header=True,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka030iFg1xua"
      },
      "source": [
        "words_dict = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    words_dict[word] = coefs\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1497_Zb5429"
      },
      "source": [
        "def sentence_word2vec(sentence):\n",
        "    vector = np.zeros(100)\n",
        "    for word in sentence.split():\n",
        "        if word in words_dict:\n",
        "            vector += words_dict[word]\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmuaU5XPNNqv"
      },
      "source": [
        "# Data Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFX3T1AnL-zJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import csv\n",
        "from scipy import sparse\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvHhwX0M3EB"
      },
      "source": [
        "data = pd.read_csv('mercari/train1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkW1GCqJxQiN",
        "outputId": "9b733577-1467-4419-ede3-d03de9ee0366"
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_id</th>\n",
              "      <th>name</th>\n",
              "      <th>item_condition_id</th>\n",
              "      <th>category_name</th>\n",
              "      <th>brand_name</th>\n",
              "      <th>price</th>\n",
              "      <th>shipping</th>\n",
              "      <th>item_description</th>\n",
              "      <th>main_category</th>\n",
              "      <th>sub_category1</th>\n",
              "      <th>sub_category2</th>\n",
              "      <th>len_name</th>\n",
              "      <th>len_item_description</th>\n",
              "      <th>name_desc</th>\n",
              "      <th>name_brand_cat</th>\n",
              "      <th>brand_value</th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>mlb cincinnati reds t shirt size xl</td>\n",
              "      <td>3</td>\n",
              "      <td>Men/Tops/T-shirts</td>\n",
              "      <td>unk_brand</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>unk_desc</td>\n",
              "      <td>Men</td>\n",
              "      <td>Tops</td>\n",
              "      <td>T-shirts</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>mlb cincinnati reds t shirt size xl unk_desc</td>\n",
              "      <td>name_mlb cincinnati reds t shirt size xl brand...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>razer blackwidow chroma keyboard</td>\n",
              "      <td>3</td>\n",
              "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
              "      <td>razer</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0</td>\n",
              "      <td>keyboard great condition works like came box p...</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>Computers &amp; Tablets</td>\n",
              "      <td>Components &amp; Parts</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>razer blackwidow chroma keyboard keyboard grea...</td>\n",
              "      <td>name_razer blackwidow chroma keyboard brandraz...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.419</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_id                                 name  item_condition_id  \\\n",
              "0         0  mlb cincinnati reds t shirt size xl                  3   \n",
              "1         1     razer blackwidow chroma keyboard                  3   \n",
              "\n",
              "                                       category_name brand_name  price  \\\n",
              "0                                  Men/Tops/T-shirts  unk_brand   10.0   \n",
              "1  Electronics/Computers & Tablets/Components & P...      razer   52.0   \n",
              "\n",
              "   shipping                                   item_description main_category  \\\n",
              "0         1                                           unk_desc           Men   \n",
              "1         0  keyboard great condition works like came box p...   Electronics   \n",
              "\n",
              "         sub_category1       sub_category2  len_name  len_item_description  \\\n",
              "0                 Tops            T-shirts         7                     1   \n",
              "1  Computers & Tablets  Components & Parts         4                    18   \n",
              "\n",
              "                                           name_desc  \\\n",
              "0       mlb cincinnati reds t shirt size xl unk_desc   \n",
              "1  razer blackwidow chroma keyboard keyboard grea...   \n",
              "\n",
              "                                      name_brand_cat  brand_value    pos  neg  \\\n",
              "0  name_mlb cincinnati reds t shirt size xl brand...            0  0.000  0.0   \n",
              "1  name_razer blackwidow chroma keyboard brandraz...            1  0.419  0.0   \n",
              "\n",
              "     neu  \n",
              "0  1.000  \n",
              "1  0.581  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCAlcFSrxQiO",
        "outputId": "d9c5e2bc-416f-4f87-efc6-dada6bcbb928"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1481661, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfwQUz5lUwQU"
      },
      "source": [
        "data.drop(columns=['train_id', 'name', 'item_description'],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3NmTbmFgmL"
      },
      "source": [
        "X = data\n",
        "y = data.price"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RTCxNq_FPOV"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFH51mnQxQiQ"
      },
      "source": [
        "brand_dict_price = X_train.groupby('brand_name').median('price')['price'].to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOgo0wTPxQiQ"
      },
      "source": [
        "price_25_percentile = np.percentile(data.price, 25)\n",
        "price_90_percentile = np.percentile(data.price, 90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmDo5VWmxQiQ"
      },
      "source": [
        "cheap_brand = set()\n",
        "affordable_brand = set()\n",
        "expensive_brand = set()\n",
        "\n",
        "def categorise_brand_price(brand_name):\n",
        "    \n",
        "    '''\n",
        "        function to categorise the brands as 'cheap', 'affordable', expensive.\n",
        "        input: brand_name\n",
        "        output: returns one category\n",
        "    '''\n",
        "    \n",
        "    if brand_dict_price[brand_name] > 0 and brand_dict_price[brand_name] <= price_25_percentile:\n",
        "        cheap_brand.add(brand_name)\n",
        "        brand = 'cheap'\n",
        "    elif brand_dict_price[brand_name] > price_25_percentile and brand_dict_price[brand_name] <= price_90_percentile:\n",
        "        affordable_brand.add(brand_name)\n",
        "        brand = 'affordable'\n",
        "    if brand_dict_price[brand_name] > price_90_percentile :\n",
        "        expensive_brand.add(brand_name)   \n",
        "        brand = 'expensive'\n",
        "    \n",
        "    return brand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isc8jRPzxQiR"
      },
      "source": [
        "# saving the file to disk\n",
        "\n",
        "pickle.dump(cheap_brand, open('cheap_brand_set', \"wb\"))\n",
        "pickle.dump(affordable_brand, open('affordable_brand_set', \"wb\"))\n",
        "pickle.dump(expensive_brand, open('expensive_brand_set', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmOztS2txQiR"
      },
      "source": [
        "def fill_brand_category(brand_name):\n",
        "    \n",
        "    '''\n",
        "        function to fill the brand_category which returns the 'cheap', 'affordable' and 'expensive'.\n",
        "    '''\n",
        "    \n",
        "    try:\n",
        "        if brand_name in cheap_brand:\n",
        "            return 'cheap'\n",
        "        elif brand_name in affordable_brand:\n",
        "            return 'affordable'\n",
        "        elif brand_name in expensive_brand:\n",
        "            return 'expensive'\n",
        "        else:\n",
        "            return 'affordable'\n",
        "    except:\n",
        "        return 'affordable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsUYqDbwxQiR"
      },
      "source": [
        "# categorise data in to cheap, affordable, expensive brands \n",
        "X_train['categorise_brand'] = X_train['brand_name'].apply(lambda x: categorise_brand_price(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro2lwRj5xQiS"
      },
      "source": [
        "X_test['categorise_brand'] = X_test['brand_name'].apply(lambda x: fill_brand_category(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz5z5Ye-xQiS"
      },
      "source": [
        "def ordinal_encoder(column_name, file_name):\n",
        "    \n",
        "    '''\n",
        "        function to give categories a unique token \n",
        "    '''\n",
        "    \n",
        "    # converting the categorical values to integer\n",
        "    ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= -1)\n",
        "    train_ = ordinal_encoder.fit_transform(X_train[column_name].astype(str).values.reshape(-1,1)).reshape(-1,1)\n",
        "    test_  = ordinal_encoder.transform(X_test[column_name].astype(str).values.reshape(-1,1)).reshape(-1,1)\n",
        "    \n",
        "    # for imputation of most_frequent values\n",
        "    imputer = SimpleImputer(missing_values=-1, strategy='most_frequent')\n",
        "    train_impute = imputer.fit_transform(train_)\n",
        "    test_impute  = imputer.transform(test_)\n",
        "    \n",
        "    \n",
        "    pickle.dump(ordinal_encoder, open('encoder/' + file_name + '_ordinal_encoder.pkl', \"wb\"))\n",
        "    pickle.dump(imputer,         open('imputer/' + file_name + '_imputer.pkl','wb'))\n",
        "\n",
        "    return train_impute, test_impute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bn2a81VxQiS"
      },
      "source": [
        "# giving each of the feature a unique label\n",
        "\n",
        "train_brand_name, test_brand_name = ordinal_encoder('brand_name', 'brand_name')\n",
        "train_category_brand, test_category_brand = ordinal_encoder('categorise_brand', 'categorise_brand')\n",
        "train_category, test_category = ordinal_encoder('category_name', 'category_name')\n",
        "train_main_category, test_main_category = ordinal_encoder('main_category', 'main_category')\n",
        "train_sub_category1, test_sub_category1 = ordinal_encoder('sub_category1', 'sub_category1')\n",
        "train_sub_category2, test_sub_category2 = ordinal_encoder('sub_category2', 'sub_category2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i6qrSyAxQiS"
      },
      "source": [
        "# additional:- https://datascience.stackexchange.com/questions/44009/scaling-label-encoded-values-for-linear-algorithms\n",
        "brand_scaler = MinMaxScaler()\n",
        "train_brand_name = brand_scaler.fit_transform(train_brand_name)\n",
        "test_brand_name =  brand_scaler.transform(test_brand_name)\n",
        "pickle.dump(brand_scaler, open('scaler/brand_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcnZeHGPxQiT"
      },
      "source": [
        "category_brand_scaler = MinMaxScaler()\n",
        "train_category_brand = category_brand_scaler.fit_transform(train_category_brand)\n",
        "test_category_brand  = category_brand_scaler.transform(test_category_brand)\n",
        "pickle.dump(category_brand_scaler, open('scaler/category_brand_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I_ofGvtxQiT"
      },
      "source": [
        "category_scaler = MinMaxScaler()\n",
        "train_category = category_scaler.fit_transform(train_category)\n",
        "test_category  = category_scaler.transform(test_category)\n",
        "pickle.dump(category_scaler, open('scaler/category_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GacOxbhxQiT"
      },
      "source": [
        "main_category_scaler = MinMaxScaler()\n",
        "train_main_category = main_category_scaler.fit_transform(train_main_category)\n",
        "test_main_category  = main_category_scaler.transform(test_main_category)\n",
        "pickle.dump(main_category_scaler, open('scaler/main_category_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zQLtxxCxQiT"
      },
      "source": [
        "main_sub_category1_scaler = MinMaxScaler()\n",
        "train_sub_category1 = main_sub_category1_scaler.fit_transform(train_sub_category1)\n",
        "test_sub_category1  = main_sub_category1_scaler.transform(test_sub_category1)\n",
        "pickle.dump(main_sub_category1_scaler, open('scaler/main_sub_category1_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSP7XqEpxQiU"
      },
      "source": [
        "main_sub_category2_scaler = MinMaxScaler()\n",
        "train_sub_category2 = main_sub_category2_scaler.fit_transform(train_sub_category2)\n",
        "test_sub_category2  = main_sub_category2_scaler.transform(test_sub_category2)\n",
        "pickle.dump(main_sub_category2_scaler, open('scaler/main_sub_category2_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ok2I9a5Hrow"
      },
      "source": [
        "train_brand_name, test_brand_name = sparse.csr_matrix(train_brand_name), sparse.csr_matrix(test_brand_name)\n",
        "train_category_brand, test_category_brand = sparse.csr_matrix(train_category_brand), sparse.csr_matrix(test_category_brand)\n",
        "train_category, test_category = sparse.csr_matrix(train_category), sparse.csr_matrix(test_category)\n",
        "train_main_category, test_main_category = sparse.csr_matrix(train_main_category), sparse.csr_matrix(test_main_category)\n",
        "train_sub_category1, test_sub_category1 = sparse.csr_matrix(train_sub_category1), sparse.csr_matrix(test_sub_category1)\n",
        "train_sub_category2, test_sub_category2 = sparse.csr_matrix(train_sub_category2), sparse.csr_matrix(test_sub_category2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slEWGe-Pk7D2"
      },
      "source": [
        "X_train.drop(columns=['brand_name','category_name', 'main_category', 'sub_category1', 'sub_category2'], inplace=True)\n",
        "X_test.drop(columns=['brand_name','category_name', 'main_category', 'sub_category1', 'sub_category2'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Vv1HegxQiU"
      },
      "source": [
        "# scaling the item_condition_id feature \n",
        "X_train['item_condition_id'] = X_train['item_condition_id'] / 5.\n",
        "X_test['item_condition_id'] = X_test['item_condition_id'] / 5."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t9xkaP81T1b"
      },
      "source": [
        "name_scaler = MinMaxScaler()\n",
        "X_train['len_name'] = name_scaler.fit_transform(X_train['len_name'].values.reshape(-1,1))\n",
        "X_test['len_name'] = name_scaler.transform(X_test['len_name'].values.reshape(-1,1))\n",
        "pickle.dump(name_scaler, open('scaler/len_name_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz4QpaGU3W3m"
      },
      "source": [
        "item_desc_scaler = MinMaxScaler()\n",
        "X_train['len_item_description'] = item_desc_scaler.fit_transform(X_train['len_item_description'].values.reshape(-1,1))\n",
        "X_test['len_item_description']  = item_desc_scaler.transform(X_test['len_item_description'].values.reshape(-1,1))\n",
        "pickle.dump(item_desc_scaler, open('scaler/len_item_description_scaler.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCtmaXWeCZtC",
        "outputId": "ff42cd6a-9112-4401-9c5e-f93b6f2b331b"
      },
      "source": [
        "print('converting the concatenation of train_name_description column to its respective vector form...')\n",
        "train_name_desc_vector = X_train.name_desc.astype(str).progress_apply(lambda x: sentence_word2vec(x))\n",
        "print('conversion of train_name_desc to vector completed!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('converting the concatenation of test_name_description column to its respective vector form...')\n",
        "test_name_desc_vector = X_test.name_desc.astype(str).progress_apply(lambda x: sentence_word2vec(x))\n",
        "print('conversion of test_name_desc to vector completed!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('converting the concatenation of name, brand, sub_categories column to its respective vector form...')\n",
        "train_name_brand_cat_vector = X_train.name_brand_cat.astype(str).progress_apply(lambda x: sentence_word2vec(x))\n",
        "print('conversion of train_name_brand_cat to vector completed!')\n",
        "print('-' * 80)\n",
        "print()\n",
        "\n",
        "print('converting the concatenation of name, brand, sub_categories column to its respective vector form...')\n",
        "test_name_brand_cat_vector = X_test.name_brand_cat.astype(str).progress_apply(lambda x: sentence_word2vec(x))\n",
        "print('conversion of test_name_brand_cat to vector completed!')\n",
        "print('-' * 80)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converting the concatenation of train_name_description column to its respective vector form...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████| 1333494/1333494 [00:40<00:00, 33120.65it/s]\n",
            "  2%|█▌                                                                   | 3300/148167 [00:00<00:04, 32997.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conversion of train_name_desc to vector completed!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "converting the concatenation of test_name_description column to its respective vector form...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████| 148167/148167 [00:04<00:00, 34053.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conversion of test_name_desc to vector completed!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "converting the concatenation of name, brand, sub_categories column to its respective vector form...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████| 1333494/1333494 [00:27<00:00, 48826.90it/s]\n",
            "  3%|██▏                                                                  | 4813/148167 [00:00<00:02, 48129.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conversion of train_name_brand_cat to vector completed!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "converting the concatenation of name, brand, sub_categories column to its respective vector form...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████| 148167/148167 [00:02<00:00, 50628.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conversion of test_name_brand_cat to vector completed!\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI1NJuonl1Og"
      },
      "source": [
        "X_train.drop(columns=['name_desc', 'name_brand_cat', 'price'], inplace=True)\n",
        "X_test.drop(columns=['name_desc', 'name_brand_cat', 'price'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ANH7FmtCZtL"
      },
      "source": [
        "train_name_desc_vector = sparse.csc_matrix(train_name_desc_vector.values.tolist())\n",
        "test_name_desc_vector = sparse.csc_matrix(test_name_desc_vector.values.tolist())\n",
        "\n",
        "train_name_brand_cat_vector = sparse.csc_matrix(train_name_brand_cat_vector.values.tolist())\n",
        "test_name_brand_cat_vector = sparse.csc_matrix(test_name_brand_cat_vector.values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgblzuApLKgf"
      },
      "source": [
        "# stacking the features horizontally\n",
        "X_train_hstack = sparse.hstack((X_train['item_condition_id'].values.reshape(-1,1),X_train['shipping'].values.reshape(-1,1),\\\n",
        "                                train_brand_name, train_category_brand, train_category, train_main_category,\\\n",
        "                                train_sub_category1, train_sub_category2, X_train['len_name'].values.reshape(-1,1),\\\n",
        "                                X_train['len_item_description'].values.reshape(-1,1), X_train['brand_value'].values.reshape(-1,1),\\\n",
        "                                X_train['pos'].values.reshape(-1,1),\\\n",
        "                                X_train['neg'].values.reshape(-1,1), X_train['neu'].values.reshape(-1,1), train_name_desc_vector, \\\n",
        "                                train_name_brand_cat_vector)).tocsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBzQ_1noLP4d"
      },
      "source": [
        "X_test_hstack  = sparse.hstack((X_test['item_condition_id'].values.reshape(-1,1),X_test['shipping'].values.reshape(-1,1),\\\n",
        "                                test_brand_name, test_category_brand, test_category, test_main_category,\\\n",
        "                                test_sub_category1, test_sub_category2, X_test['len_name'].values.reshape(-1,1),\\\n",
        "                                X_test['len_item_description'].values.reshape(-1,1), X_test['brand_value'].values.reshape(-1,1),\\\n",
        "                                 X_test['pos'].values.reshape(-1,1),\\\n",
        "                                X_test['neg'].values.reshape(-1,1), X_test['neu'].values.reshape(-1,1), test_name_desc_vector, \\\n",
        "                                test_name_brand_cat_vector)).tocsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXHuVjgULisQ"
      },
      "source": [
        "sparse.save_npz('mercari/X_train_hstack.npz', X_train_hstack)\n",
        "sparse.save_npz('mercari/X_test_hstack.npz', X_test_hstack)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y35S_PldxQiY"
      },
      "source": [
        "y_train = sparse.csc_matrix(y_train).reshape(-1,1)\n",
        "y_test = sparse.csc_matrix(y_test).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqzkjwDUxQiY"
      },
      "source": [
        "sparse.save_npz('mercari/y_train.npz', y_train)\n",
        "sparse.save_npz('mercari/y_test.npz', y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}